Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@techreport{Tyagi2007,
abstract = {We present a computer vision system for robust object tracking in 3D by combining evidence from multiple cali-brated cameras. This kernel-based 3D tracker is automati-cally bootstrapped by constructing 3D point clouds. These points clouds are then clustered and used to initialize the trackers and validate their performance. The framework describes a complete tracking system that fuses appearance features from all available camera sensors and is capable of automatic initialization and drift detection. Its elegance re-sides in its inherent ability to handle problems encountered by various 2D trackers, including scale selection, occlusion, view-dependence, and correspondence across views. Track-ing results for an indoor smart room and a multi-camera outdoor surveillance scenario are presented. We demon-strate the effectiveness ofthis unified approach by compar-ing its performance to a baseline 3D tracker that fuses re-sults ofindependent 2D trackers, as well as comparing the re-initialization results to known ground truth.},
author = {Tyagi, A and Keck, M and {\ldots}, JW Davis - 2007 IEEE Conference and 2007, Undefined},
booktitle = {IEEEWorkshop on Visual Surveillance},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tyagi et al. - Unknown - Kernel-based 3d tracking.pdf:pdf},
title = {{Kernel-based 3d tracking}},
url = {https://ieeexplore.ieee.org/abstract/document/4270499/},
year = {2007}
}
@inproceedings{Lai2010,
abstract = {$\backslash$nIn recent years, object detection has become an increasingly active field of research in robotics. An important problem in object detection is the availability of a sufficient amount of labeled training data to learn good classifiers. In this paper we show how to significantly reduce the need for manually labeled training data by leveraging data sets available on the World Wide Web. Specifically, we show how to use objects from Google's 3D Warehouse to train an object detection system for 3D point clouds collected by robots navigating through both urban and indoor environments. In order to deal with the different characteristics of the web data and the real robot data, we additionally use a small set of labeled point clouds and perform domain adaptation. Our experiments demonstrate that additional data taken from the 3D Warehouse along with our domain adaptation greatly improves the classification accuracy on real-world environments.$\backslash$n},
author = {Lai, Kevin and Fox, Dieter},
booktitle = {International Journal of Robotics Research},
doi = {10.1177/0278364910369190},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/object-recognition-in-3d-point-clouds-using-web-data-and-domain-adaptation.pdf:pdf},
issn = {02783649},
month = {jul},
number = {8},
pages = {1019--1037},
title = {{Object recognition in 3D point clouds using web data and domain adaptation}},
url = {http://journals.sagepub.com/doi/10.1177/0278364910369190},
volume = {29},
year = {2010}
}
@inproceedings{Sidenbladh2000,
abstract = {A probabilistic method for tracking 3D articulated human figures in monocular image sequences is presented. Within a Bayesian framework, we define a generative model of image appearance, a robust likelihood function based on image graylevel differences, and a prior probability distribution over pose and joint angles that models how humans move. The posterior probability distribution over model parameters is represented using a discrete set of samples and is propagated over time using particle filtering. The approach extends previous work on parameterized optical flow estimation to exploit a complex 3D articulated motion model. It also extends previous work on human motion tracking by including a perspective camera model, by modeling limb self occlusion, and by recovering 3D motion from a monocular sequence. The explicit posterior probability distribution represents ambiguities due to image matching, model singularities, and perspective projection. The method relies only on a frame-to-frame assumption of brightness constancy and hence is able to track people under changing viewpoints, in grayscale image sequences, and with complex unknown backgrounds.},
author = {Sidenbladh, Hedvig and Black, Michael J. and Fleet, David J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidenbladh et al. - Unknown - Stochastic tracking of 3D human figures using 2D image motion.pdf:pdf},
isbn = {3540676864},
issn = {16113349},
pages = {702--718},
title = {{Stochastic tracking of 3D human figures using 2D image motion}},
url = {https://link.springer.com/chapter/10.1007/3-540-45053-X{\_}45},
volume = {1843},
year = {2000}
}
@article{Howe1999,
abstract = {The three-dimensional motion of humans is underdetermined when the observation is limited to a single camera, due to the inherent 3D ambiguity of 2D video. We present a system that reconstructs the 3D motion of human sub jects from single-camera video, relying on prior knowledge about human motion, learned from training data, to resolve those ambiguities. After initialization in 2D, the tracking and 3D reconstruction is automatic; we show results for several video sequences. The results show the power of treating 3D body tracking as an inference problem.},
author = {Howe, NR and Leventon, ME and Freeman, W.},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Howe et al. - Unknown - Bayesian reconstruction of 3d human motion from single-camera video.pdf:pdf},
isbn = {0-262-19450-3},
issn = {1049-5258},
journal = {Neural Information Processing Systems},
pages = {1},
title = {{Bayesian reconstruction of 3d human motion from single-camera video}},
url = {http://papers.nips.cc/paper/1698-bayesian-reconstruction-of-3d-human-motion-from-single-camera-video.pdf http://www.merl.com/papers/docs/TR1999-037.pdf},
volume = {1999},
year = {1999}
}
@article{Niemeyer,
abstract = {In this work we address the task of the contextual classification of an airborne LiDAR point cloud. For that purpose, we integrate a Random Forest classifier into a Conditional Random Field (CRF) framework. It is a flexible approach for obtaining a reliable classification result even in complex urban scenes. In this way, we benefit from the consideration of context on the one hand and from the opportunity to use a large amount of features on the other hand. Considering the interactions in our experiments increases the overall accuracy by 2{\%}, though a larger improvement becomes apparent in the completeness and correctness of some of the seven classes discerned in our experiments. We compare the Random Forest approach to linear models for the computation of unary and pairwise potentials of the CRF, and investigate the relevance of different features for the LiDAR points as well as for the interaction of neighbouring points. In a second step, building objects are detected based on the classified point cloud. For that purpose, the CRF probabilities for the classes are plugged into a Markov Random Field as unary potentials, in which the pairwise potentials are based on a Potts model. The 2D binary building object masks are extracted and evaluated by the benchmark ISPRS Test Project on Urban Classification and 3D Building Reconstruction. The evaluation shows that the main buildings (larger than 50m2) can be detected very reliably with a correctness larger than 96{\%} and a completeness of 100{\%}. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Niemeyer, Joachim and Rottensteiner, Franz and Soergel, Uwe},
doi = {10.1016/j.isprsjprs.2013.11.001},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niemeyer, Rottensteiner, Soergel - 2014 - Contextual classification of lidar data and building object detection in urban areas.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building,Classification,Contextual,Detection,LIDAR,Point cloud,Urban,database,dataset},
mendeley-tags = {database,dataset},
pages = {152--165},
title = {{Contextual classification of lidar data and building object detection in urban areas}},
url = {https://www.sciencedirect.com/science/article/pii/S0924271613002359},
volume = {87},
year = {2014}
}
@inproceedings{Khokhlova2018,
abstract = {{\textcopyright} 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved. This paper introduces a simple yet powerful algorithm for global human posture description based on 3D Point Cloud data. The proposed algorithm preserves spatial contextual information about a 3D object in a video sequence and can be used as an intermediate step in human-motion related Computer Vision applications such as action recognition, gait analysis, human-computer interaction. The proposed descriptor captures a point cloud structure by means of a modified 3D regular grid and a corresponding cells space occupancy information. The performance of our method was evaluated on the task of posture recognition and automatic action segmentation.},
author = {Khokhlova, Margarita and Migniot, Cyrille and Dipanda, Albert},
booktitle = {VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khokhlova et al. - Unknown - 3D Point Cloud Descriptor for Posture Recognition.pdf:pdf},
isbn = {9789897582905},
keywords = {3D descriptor,3D posture,Point cloud structure},
pages = {161--168},
title = {{3D point cloud descriptor for posture recognition}},
url = {https://www.scitepress.org/Papers/2018/65418/65418.pdf},
volume = {5},
year = {2018}
}
@article{Vallet2015,
abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
author = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
doi = {10.1016/j.cag.2015.03.004},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/terraMobilita-iqmulus-urban-point-cloud-analysis-benchmark.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene,database,dataset},
mendeley-tags = {database,dataset},
month = {jun},
pages = {126--133},
publisher = {Pergamon},
title = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S009784931500028X},
volume = {49},
year = {2015}
}
@inproceedings{Munoz2009,
abstract = {We address the problem of label assignment in computer vision: given a novel 3D or 2D scene, we wish to assign a unique label to every site (voxel, pixel, superpixel, etc.). To this end, the Markov Random Field framework has proven to be a model of choice as it uses contextual information to yield improved classification results over locally independent classifiers. In this work we adapt a functional gradient approach for learning high-dimensional parameters of random fields in order to perform discrete, multi-label classification. With this approach we can learn robust models involving high-order interactions better than the previously used learning method. We validate the approach in the context of point cloud classification and improve the state of the art. In addition, we successfully demonstrate the generality of the approach on the challenging vision problem of recovering 3-D geometric surfaces from images.},
author = {Munoz, Daniel and Bagnell, J. Andrew and Vandapel, Nicolas and Hebert, Martial},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206590},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/contextual-classification-with-functional-max-margin-markov-networks.pdf:pdf},
isbn = {9781424439935},
keywords = {database,dataset},
mendeley-tags = {database,dataset},
pages = {975--982},
title = {{Contextual classification with functional max-margin markov networks}},
url = {https://www.google.com/search?client=ubuntu{\&}channel=fs{\&}q=Contextual+Classification+with+Functional+Max-Margin+Markov+Networks{\&}ie=utf-8{\&}oe=utf-8},
volume = {2009 IEEE},
year = {2009}
}
@article{Ariz2016,
abstract = {A new public database of videos for head tracking and pose estimation is presented in this paper with the goal of establishing a new framework for algorithm validation, replacing out of date frameworks. Position data has been recorded with a magnetic sensor-transmitter that has previously been aligned and synchronized with a commercial webcam, and we provide reliable ground-truth for 3D rotation and translation of the head with respect to the camera. In addition to this, an automatic face annotation procedure has been developed, which provides the image position of 54 facial landmarks, with negligible error, in every video frame in the database. This image ground-truth can be used for algorithm training or head tracking evaluation, among others. In order to show the usability of the database, we evaluate three head tracking approaches and three head models, and combine them to provide nine different head pose estimation sets of results. We show the validity of the presented database both for training and evaluation of head tracking and pose estimation methods, and provide an interesting comparison in performance of state-of-the-art algorithms. These results may also serve as reference to encourage other researchers to train and test their algorithms with this database, and compare their results with the ones presented in this paper.},
author = {Ariz, Mikel and Bengoechea, Jos{\'{e}} J. and Villanueva, Arantxa and Cabeza, Rafael},
doi = {10.1016/j.cviu.2015.04.009},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/a-novel-2d3d-database-with-automatic-face-annotation-for-head-tracking-and-pose-estimation.pdf:pdf},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {AAM,ASM,Automatic face annotation,Head model,Head pose estimation,Head tracking,POSIT},
pages = {201--210},
title = {{A novel 2D/3D database with automatic face annotation for head tracking and pose estimation}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314215000934},
volume = {148},
year = {2016}
}
@inproceedings{Hackel,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
author = {Hackel, T and Savinov, N and Ladicky, L and Wegner, J. D. and Schindler, K. and Pollefeys, M.},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-91-2017},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hackel et al. - 2017 - SEMANTIC3D.NET A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK.pdf:pdf},
issn = {21949050},
keywords = {database,dataset},
mendeley-tags = {database,dataset},
number = {1W1},
pages = {91--98},
title = {{SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK}},
url = {https://arxiv.org/abs/1704.03847},
volume = {4},
year = {2017}
}
@inproceedings{Firman2016,
abstract = {Since the launch of the Microsoft Kinect, scores of RGBD datasets have been released. These have propelled advances in areas from reconstruction to gesture recognition. In this paper we explore the field, reviewing datasets across eight categories: semantics, object pose estimation, camera tracking, scene reconstruction, object tracking, human actions, faces and identification. By extracting relevant information in each category we help researchers to find appropriate data for their needs, and we consider which datasets have succeeded in driving computer vision forward and why. Finally, we examine the future of RGBD datasets. We identify key areas which are currently underexplored, and suggest that future directions may include synthetic data and dense reconstructions of static and dynamic scenes.},
author = {Firman, Michael},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.88},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/computer, 2016 - Unknown - RGBD datasets Past, present and future.pdf:pdf},
isbn = {9781467388504},
issn = {21607516},
number = {Section 3},
pages = {661--673},
title = {{RGBD Datasets: Past, Present and Future}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016{\_}workshops/w17/html/Firman{\_}RGBD{\_}Datasets{\_}Past{\_}CVPR{\_}2016{\_}paper.html},
year = {2016}
}
@inproceedings{Cherabier2016,
abstract = {Techniques that jointly perform dense 3D reconstruction and semantic segmentation have recently shown very promising results. One major restriction so far is that they can often only handle a very low number of semantic labels. This is mostly due to their high memory consumption caused by the necessity to store indicator variables for every label and transition. We propose a way to reduce the memory consumption of existing methods. Our approach is based on the observation that many semantic labels are only present at very localized positions in the scene, such as cars. Therefore this label does not need to be active at every location. We exploit this observation by dividing the scene into blocks in which generally only a subset of labels is active. By determining early on in the reconstruction process which labels need to be active in which block the memory consumption can be significantly reduced. In order to recover from mistakes we propose to update the set of active labels during the iterative optimization procedure based on the current solution. We also propose a way to initialize the set of active labels using a boosted classifier. In our experimental evaluation we show the reduction of memory usage quantitatively. Eventually, we show results of joint semantic 3D reconstruction and semantic segmentation with significantly more labels than previous approaches were able to handle.},
author = {Cherabier, Ian and Hane, Christian and Oswald, Martin R. and Pollefeys, Marc},
booktitle = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
doi = {10.1109/3DV.2016.68},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - Unknown - Pointnet Deep learning on point sets for 3d classification and segmentation.pdf:pdf},
isbn = {9781509054077},
keywords = {dense 3D reconstruction,semantic segmentation},
pages = {601--610},
title = {{Multi-label semantic 3D reconstruction using voxel blocks}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/html/Qi{\_}PointNet{\_}Deep{\_}Learning{\_}CVPR{\_}2017{\_}paper.html},
year = {2016}
}
@article{Shi2006,
abstract = {A random forest (RF) predictor is an ensemble of$\backslash$nindividual tree predictors. As part of their construction,$\backslash$nRF predictors naturally lead to a dissimilarity measure$\backslash$nbetween the observations. One can also define an RF$\backslash$ndissimilarity measure between unlabeled data: the idea is$\backslash$nto construct an RF predictor that distinguishes the$\backslash$nobserved data from suitably generated synthetic data. The$\backslash$nobserved data are the original unlabeled data and the$\backslash$nsynthetic data are drawn from a reference distribution.$\backslash$nHere we describe the properties of the RF dissimilarity and$\backslash$nmake recommendations on how to use it in practice. An RF$\backslash$ndissimilarity can be attractive because it handles mixed$\backslash$nvariable types well, is invariant to monotonic$\backslash$ntransformations of the input variables, and is robust to$\backslash$noutlying observations. The RF dissimilarity easily deals$\backslash$nwith a large number of variables due to its intrinsic$\backslash$nvariable selection; for example, the Addcl 1 RF$\backslash$ndissimilarity weighs the contribution of each variable$\backslash$naccording to how dependent it is on other variables. We$\backslash$nfind that the RF dissimilarity is useful for detecting$\backslash$ntumor sample clusters on the basis of tumor marker$\backslash$nexpressions. In this application, biologically meaningful$\backslash$nclusters can often be described with simple thresholding$\backslash$nrules.},
author = {Shi, Tao and Horvath, Steve},
doi = {10.1198/106186006X94072},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/unsupervised-learning-with-random-forest-predictors.pdf:pdf},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Biomarkers,Cluster analysis,Dissimilarity,Ensemble predictors,Tumor markers},
month = {mar},
number = {1},
pages = {118--138},
title = {{Unsupervised learning with random forest predictors}},
url = {https://www.tandfonline.com/doi/full/10.1198/106186006X94072},
volume = {15},
year = {2006}
}
@inproceedings{Rougier2006,
abstract = {Faced with the growing population of seniors, Western societies need to think about new technologies to ensure the safety of elderly people at home. Computer vision provides a good solution for healthcare systems because it allows a specific analysis of people behavior. Moreover, a system based on video surveillance is particularly well adapted to detect falls. We present a new method to detect falls using a single camera. Our approach is based on the 3D trajectory of the head, which allows us to distinguish falls from normal activities using 3D velocities. {\textcopyright} 2006 IEEE.},
author = {Rougier, Caroline and Meunier, Jean and St-Arnaud, Alain and Rousseau, Jacqueline},
booktitle = {Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
doi = {10.1109/IEMBS.2006.260829},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rougier et al. - Unknown - Monocular 3D head tracking to detect falls of elderly people.pdf:pdf},
isbn = {1424400325},
issn = {05891019},
pages = {6384--6387},
title = {{Monocular 3D head tracking to detect falls of elderly people}},
url = {https://ieeexplore.ieee.org/abstract/document/4463271/},
year = {2006}
}
@inproceedings{Held2013,
abstract = {Precision tracking is important for predicting the behavior of other$\backslash$ncars in autonomous driving. We present a novel method to combine$\backslash$nlaser and camera data to achieve accurate velocity estimates of moving$\backslash$nvehicles. We combine sparse laser points with a high-resolution camera$\backslash$nimage to obtain a dense colored point cloud. We use a color-augmented$\backslash$nsearch algorithm to align the dense color point clouds from successive$\backslash$ntime frames for a moving vehicle, thereby obtaining a precise estimate$\backslash$nof the tracked vehicle's velocity. Using this alignment method, we$\backslash$nobtain velocity estimates at a much higher accuracy than previous$\backslash$nmethods. Through pre-filtering, we are able to achieve near real$\backslash$ntime results. We also present an online method for real-time use$\backslash$nwith accuracies close to that of the full method. We present a novel$\backslash$napproach to quantitatively evaluate our velocity estimates by tracking$\backslash$na parked car in a local reference frame in which it appears to be$\backslash$nmoving relative to the ego vehicle. We use this evaluation method$\backslash$nto automatically quantitatively evaluate our tracking performance$\backslash$non 466 separate tracked vehicles. Our method obtains a mean absolute$\backslash$nvelocity error of 0.27 m/s and an RMS error of 0.47 m/s on this test$\backslash$nset. We can also qualitatively evaluate our method by building color$\backslash$n3D car models from moving vehicles. We have thus demonstrated that$\backslash$nour method can be used for precision car tracking with applications$\backslash$nto autonomous driving and behavior modeling.},
author = {Held, David and Levinson, Jesse and Thrun, Sebastian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630715},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - Unknown - Precision tracking with sparse 3d and dense color 2d data.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
pages = {1138--1145},
title = {{Precision tracking with sparse 3D and dense color 2D data}},
url = {https://ieeexplore.ieee.org/abstract/document/6630715/},
year = {2013}
}
@article{Knoop2009,
abstract = {In this article, we present an approach for the fusion of 2d and 3d measurements for model-based person tracking, also known as Human Motion Capture. The applied body model is defined geometrically with generalized cylinders, and is set up hierarchically with connecting joints of different types. The joint model can be parameterized to control the degrees of freedom, adhesion and stiffness. This results in an articulated body model with constrained kinematic degrees of freedom. The fusion approach incorporates this model knowledge together with the measurements, and tracks the target body iteratively with an extended Iterative Closest Point (ICP) approach. Generally, the ICP is based on the concept of correspondences between measurements and model, which is normally exploited to incorporate 3d point cloud measurements. The concept has been generalized to represent and incorporate also 2d image space features. Together with the 3D point cloud from a 3d time-of-flight (ToF) camera, arbitrary features, derived from 2D camera images, are used in the fusion algorithm for tracking of the body. This gives complementary information about the tracked body, enabling not only tracking of depth motions but also turning movements of the human body, which is normally a hard problem for markerless human motion capture systems. The resulting tracking system, named VooDoo is used to track humans in a Human-Robot Interaction (HRI) context. We only rely on sensors on board the robot, i.e. the color camera, the ToF camera and a laser range finder. The system runs in realtime (∼20 Hz) and is able to robustly track a human in the vicinity of the robot. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Knoop, Steffen and Vacek, Stefan and Dillmann, R{\"{u}}diger},
doi = {10.1016/j.robot.2008.10.017},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/fusion-of-2d-and-3d-sensor-data-for-articulated-body-tracking.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {3D body model,Human motion capture,Human robot interaction,Sensor fusion,Time-of-flight},
number = {3},
pages = {321--329},
title = {{Fusion of 2d and 3d sensor data for articulated body tracking}},
url = {https://www.sciencedirect.com/science/article/pii/S0921889008001711},
volume = {57},
year = {2009}
}
@inproceedings{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_54},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silberman et al. - 2012 - Indoor segmentation and support inference from RGBD images.pdf:pdf},
isbn = {9783642337147},
issn = {03029743},
keywords = {database,dataset},
mendeley-tags = {database,dataset},
number = {PART 5},
pages = {746--760},
title = {{Indoor segmentation and support inference from RGBD images}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-33715-4{\_}54},
volume = {7576 LNCS},
year = {2012}
}
@inproceedings{Koppula2011,
abstract = {RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic la- beling of these 3D point clouds. We propose a graphical model that captures var- ious features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the models parsimony becomes im- portant and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06{\%} in labeling 17 object classes for offices, and 73.38{\%} in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms},
author = {Koppula, Hema Swetha and Anand, Abhishek and Joachims, Thorsten and Saxena, Ashutosh},
booktitle = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppula et al. - Unknown - Semantic labeling of 3d point clouds for indoor scenes.pdf:pdf},
isbn = {9781618395993},
title = {{Semantic labeling of 3D point clouds for indoor scenes}},
url = {http://papers.nips.cc/paper/4226-semantic-labeling-of-3d-point-clouds-for-indoor-scenes},
year = {2011}
}
@inproceedings{Hegger2013,
abstract = {The ability to detect people in domestic and unconstrained environments is crucial for every service robot. The knowledge where people are is required to perform several tasks such as navigation with dynamic obstacle avoidance and human-robot-interaction. In this paper we propose a people detection approach based on 3d data provided by a RGB-D camera. We introduce a novel 3d feature descriptor based on Local Surface Normals (LSN) which is used to learn a classifier in a supervised machine learning manner. In order to increase the systems flexibility and to detect people even under partial occlusion we introduce a top-down/bottom-up segmentation. We deployed the people detection system on a real-world service robot operating at a reasonable frame rate of 5Hz. The experimental results show that our approach is able to detect persons in various poses and motions such as sitting, walking, and running.},
author = {Hegger, Frederik and Hochgeschwender, Nico and Kraetzschmar, Gerhard K. and Ploeger, Paul G.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39250-4_15},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/people-detection-in-3d-point-clouds-using-local-surface-normals.pdf:pdf},
isbn = {9783642392498},
issn = {03029743},
keywords = {Human-Robot Interaction,People Detection,RGB-D},
pages = {154--164},
title = {{People detection in 3d point clouds using local surface normals}},
url = {http://link.springer.com/10.1007/978-3-642-39250-4{\_}15},
volume = {7500 LNAI},
year = {2013}
}
@article{JoseM.Chaquet2013,
author = {{Jos{\'{e}} M. Chaquet} and {Enrique J. Carmona} and {Antonio Fern{\'{a}}ndez-Caballero}},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaquet, {\ldots}, 2013 - Unknown - A survey of video datasets for human action and activity recognition.pdf:pdf},
journal = {Computer Vision and Image Understanding},
number = {6},
pages = {633--659},
title = {{A survey of video datasets for human action and activity recognition}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314213000295},
volume = {117},
year = {2013}
}
