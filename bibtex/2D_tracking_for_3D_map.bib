Automatically generated by Mendeley Desktop 1.19.5
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Choi2009,
abstract = {In this paper we present a new framework for pedestrian action categorization. Our method enables the classification of actions whose semantic can be only analyzed by looking at the collective behavior of pedestrians in the scene. Examples of these actions are waiting by a street intersection versus standing in a queue. To that end, we exploit the spatial distribution of pedestrians in the scene as well as their pose and motion for achieving robust action classification. Our proposed solution employs extended Kalman filtering for tracking of detected pedestrians in 2D 1/2 scene coordinates as well as camera parameter and horizon estimation for tracker filtering and stabilization. We present a local spatio-temporal descriptor effective in capturing the spatial distribution of pedestrians over time as well as their pose. This descriptor captures pedestrian activity while requiring no high level scene understanding. Our work is tested against highly challenging real world pedestrian video sequences captured by low resolution hand held cameras. Experimental results on a 5-class action dataset indicate that our solution: i) is effective in classifying collective pedestrian activities; ii) is tolerant to challenging real world conditions such as variation in illumination, scale, viewpoint as well as partial occlusion and background motion; iii) outperforms state-of-the art action classification techniques.},
author = {Choi, Wongun and Shahid, Khuram and Savarese, Silvio},
booktitle = {2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops 2009},
doi = {10.1109/ICCVW.2009.5457461},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/Wongun{\_}CollectiveActivityRecognition09.pdf:pdf},
isbn = {9781424444427},
keywords = {2D,dynamic,human,scene,synched},
mendeley-tags = {human,scene,dynamic,synched,2D},
month = {sep},
pages = {1282--1289},
publisher = {IEEE},
title = {{What are they doing?: Collective activity classification using spatio-temporal relationship among people}},
url = {http://ieeexplore.ieee.org/document/5457461/},
year = {2009}
}
@techreport{Tyagi2007,
abstract = {We present a computer vision system for robust object tracking in 3D by combining evidence from multiple cali-brated cameras. This kernel-based 3D tracker is automati-cally bootstrapped by constructing 3D point clouds. These points clouds are then clustered and used to initialize the trackers and validate their performance. The framework describes a complete tracking system that fuses appearance features from all available camera sensors and is capable of automatic initialization and drift detection. Its elegance re-sides in its inherent ability to handle problems encountered by various 2D trackers, including scale selection, occlusion, view-dependence, and correspondence across views. Track-ing results for an indoor smart room and a multi-camera outdoor surveillance scenario are presented. We demon-strate the effectiveness ofthis unified approach by compar-ing its performance to a baseline 3D tracker that fuses re-sults ofindependent 2D trackers, as well as comparing the re-initialization results to known ground truth.},
author = {Tyagi, A and Keck, M and {\ldots}, JW Davis - 2007 IEEE Conference and 2007, Undefined},
booktitle = {IEEEWorkshop on Visual Surveillance},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tyagi et al. - Unknown - Kernel-based 3d tracking.pdf:pdf},
title = {{Kernel-based 3d tracking}},
url = {https://ieeexplore.ieee.org/abstract/document/4270499/},
year = {2007}
}
@incollection{Kepski2012,
abstract = {In this paper we demonstrate how to accomplish reliable fall detection on a low-cost embedded platform. The detection is achieved by a fuzzy inference system using Kinect and a wearable motion-sensing device that consists of accelerometer and gyroscope. The foreground objects are detected using depth images obtained by Kinect, which is able to extract such images in a room that is dark to our eyes. The system has been implemented on the PandaBoard ES and runs in real-time. It permits unobtrusive fall detection as well as preserves privacy of the user. The experimental results indicate high effectiveness of fall detection.},
author = {Kepski, Michal and Kwolek, Bogdan},
booktitle = {Elsevier},
doi = {10.1007/978-3-642-31534-3_60},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwolek, biomedicine, 2014 - Unknown - Human fall detection on embedded platform using depth maps and wireless accelerometer.pdf:pdf},
keywords = {dataset,dynamic,falling,human,scene,synched},
mendeley-tags = {dataset,dynamic,falling,human,scene,synched},
pages = {407--414},
title = {{Fall Detection on Embedded Platform Using Kinect and Wireless Accelerometer}},
url = {https://www.sciencedirect.com/science/article/pii/S0169260714003447},
year = {2012}
}
@inproceedings{Zhang2018,
abstract = {{\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature. Human activity understanding from RGB-D data has attracted increasing attention since the first work reported in 2010. Over this period, many benchmark datasets have been created to facilitate the development and evaluation of new algorithms. However, the existing datasets are mostly captured in laboratory environment with small number of actions and small variations, which impede the development of higher level algorithms for real world applications. Thus, this paper proposes a large scale dataset along with a set of evaluation protocols. The large dataset is created by combining several existing publicly available datasets and can be expanded easily by adding more datasets. The large dataset is suitable for testing algorithms from different perspectives using the proposed evaluation protocols. Four state-of-the-art algorithms are evaluated on the large combined dataset and the results have verified the limitations of current algorithms and the effectiveness of the large dataset.},
author = {Zhang, Jing and Li, Wanqing and Wang, Pichao and Ogunbona, Philip and Liu, Song and Tang, Chang},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-91863-1_8},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - A large scale rgb-d dataset for action recognition.pdf:pdf},
isbn = {9783319918624},
issn = {16113349},
keywords = {Action recognition,Evaluation protocol,Large scale RGB-D dataset,action,dynamic,human,synched},
mendeley-tags = {action,dynamic,human,synched},
pages = {101--114},
title = {{A large scale RGB-D dataset for action recognition}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-91863-1{\_}8},
volume = {10188 LNCS},
year = {2018}
}
@inproceedings{Ofli2013,
author = {Ofli, Ferda and Chaudhry, Rizwan and Kurillo, Gregorij},
booktitle = {Applications of Computer Vision (WACV), 2013 IEEE Workshop on},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ofli et al. - Unknown - Berkeley mhad A comprehensive multimodal human action database.pdf:pdf},
keywords = {action,database,dataset,human},
mendeley-tags = {action,database,dataset,human},
pages = {53--60},
title = {{Berkeley Multimodal Human Action Database (MHAD)}},
url = {https://ieeexplore.ieee.org/abstract/document/6474999/ http://tele-immersion.citris-uc.org/berkeley{\_}mhad/},
year = {2013}
}
@inproceedings{Lai2010,
abstract = {$\backslash$nIn recent years, object detection has become an increasingly active field of research in robotics. An important problem in object detection is the availability of a sufficient amount of labeled training data to learn good classifiers. In this paper we show how to significantly reduce the need for manually labeled training data by leveraging data sets available on the World Wide Web. Specifically, we show how to use objects from Google's 3D Warehouse to train an object detection system for 3D point clouds collected by robots navigating through both urban and indoor environments. In order to deal with the different characteristics of the web data and the real robot data, we additionally use a small set of labeled point clouds and perform domain adaptation. Our experiments demonstrate that additional data taken from the 3D Warehouse along with our domain adaptation greatly improves the classification accuracy on real-world environments.$\backslash$n},
author = {Lai, Kevin and Fox, Dieter},
booktitle = {International Journal of Robotics Research},
doi = {10.1177/0278364910369190},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/object-recognition-in-3d-point-clouds-using-web-data-and-domain-adaptation.pdf:pdf},
issn = {02783649},
keywords = {database,dataset,object,scene,synched},
mendeley-tags = {database,dataset,object,scene,synched},
month = {jul},
number = {8},
pages = {1019--1037},
title = {{Object recognition in 3D point clouds using web data and domain adaptation}},
url = {http://journals.sagepub.com/doi/10.1177/0278364910369190},
volume = {29},
year = {2010}
}
@inproceedings{Sidenbladh2000,
abstract = {A probabilistic method for tracking 3D articulated human figures in monocular image sequences is presented. Within a Bayesian framework, we define a generative model of image appearance, a robust likelihood function based on image graylevel differences, and a prior probability distribution over pose and joint angles that models how humans move. The posterior probability distribution over model parameters is represented using a discrete set of samples and is propagated over time using particle filtering. The approach extends previous work on parameterized optical flow estimation to exploit a complex 3D articulated motion model. It also extends previous work on human motion tracking by including a perspective camera model, by modeling limb self occlusion, and by recovering 3D motion from a monocular sequence. The explicit posterior probability distribution represents ambiguities due to image matching, model singularities, and perspective projection. The method relies only on a frame-to-frame assumption of brightness constancy and hence is able to track people under changing viewpoints, in grayscale image sequences, and with complex unknown backgrounds.},
author = {Sidenbladh, Hedvig and Black, Michael J. and Fleet, David J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidenbladh et al. - Unknown - Stochastic tracking of 3D human figures using 2D image motion.pdf:pdf},
isbn = {3540676864},
issn = {16113349},
pages = {702--718},
title = {{Stochastic tracking of 3D human figures using 2D image motion}},
url = {https://link.springer.com/chapter/10.1007/3-540-45053-X{\_}45},
volume = {1843},
year = {2000}
}
@article{Howe1999,
abstract = {The three-dimensional motion of humans is underdetermined when the observation is limited to a single camera, due to the inherent 3D ambiguity of 2D video. We present a system that reconstructs the 3D motion of human sub jects from single-camera video, relying on prior knowledge about human motion, learned from training data, to resolve those ambiguities. After initialization in 2D, the tracking and 3D reconstruction is automatic; we show results for several video sequences. The results show the power of treating 3D body tracking as an inference problem.},
author = {Howe, NR and Leventon, ME and Freeman, W.},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Howe et al. - Unknown - Bayesian reconstruction of 3d human motion from single-camera video.pdf:pdf},
isbn = {0-262-19450-3},
issn = {1049-5258},
journal = {Neural Information Processing Systems},
pages = {1},
title = {{Bayesian reconstruction of 3d human motion from single-camera video}},
url = {http://papers.nips.cc/paper/1698-bayesian-reconstruction-of-3d-human-motion-from-single-camera-video.pdf http://www.merl.com/papers/docs/TR1999-037.pdf},
volume = {1999},
year = {1999}
}
@article{Niemeyer2014,
abstract = {In this work we address the task of the contextual classification of an airborne LiDAR point cloud. For that purpose, we integrate a Random Forest classifier into a Conditional Random Field (CRF) framework. It is a flexible approach for obtaining a reliable classification result even in complex urban scenes. In this way, we benefit from the consideration of context on the one hand and from the opportunity to use a large amount of features on the other hand. Considering the interactions in our experiments increases the overall accuracy by 2{\%}, though a larger improvement becomes apparent in the completeness and correctness of some of the seven classes discerned in our experiments. We compare the Random Forest approach to linear models for the computation of unary and pairwise potentials of the CRF, and investigate the relevance of different features for the LiDAR points as well as for the interaction of neighbouring points. In a second step, building objects are detected based on the classified point cloud. For that purpose, the CRF probabilities for the classes are plugged into a Markov Random Field as unary potentials, in which the pairwise potentials are based on a Potts model. The 2D binary building object masks are extracted and evaluated by the benchmark ISPRS Test Project on Urban Classification and 3D Building Reconstruction. The evaluation shows that the main buildings (larger than 50m2) can be detected very reliably with a correctness larger than 96{\%} and a completeness of 100{\%}. {\textcopyright} 2013 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).},
author = {Niemeyer, Joachim and Rottensteiner, Franz and Soergel, Uwe},
doi = {10.1016/j.isprsjprs.2013.11.001},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niemeyer, Rottensteiner, Soergel - 2014 - Contextual classification of lidar data and building object detection in urban areas.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Building,Classification,Contextual,Detection,LIDAR,Point cloud,Urban,database,dataset,georeferenced,map,synched},
mendeley-tags = {database,dataset,georeferenced,map,synched},
pages = {152--165},
title = {{Contextual classification of lidar data and building object detection in urban areas}},
url = {https://www.sciencedirect.com/science/article/pii/S0924271613002359},
volume = {87},
year = {2014}
}
@inproceedings{Khokhlova2018,
abstract = {{\textcopyright} 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved. This paper introduces a simple yet powerful algorithm for global human posture description based on 3D Point Cloud data. The proposed algorithm preserves spatial contextual information about a 3D object in a video sequence and can be used as an intermediate step in human-motion related Computer Vision applications such as action recognition, gait analysis, human-computer interaction. The proposed descriptor captures a point cloud structure by means of a modified 3D regular grid and a corresponding cells space occupancy information. The performance of our method was evaluated on the task of posture recognition and automatic action segmentation.},
author = {Khokhlova, Margarita and Migniot, Cyrille and Dipanda, Albert},
booktitle = {VISIGRAPP 2018 - Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Khokhlova et al. - Unknown - 3D Point Cloud Descriptor for Posture Recognition.pdf:pdf},
isbn = {9789897582905},
keywords = {3D descriptor,3D posture,Point cloud structure},
pages = {161--168},
title = {{3D point cloud descriptor for posture recognition}},
url = {https://www.scitepress.org/Papers/2018/65418/65418.pdf},
volume = {5},
year = {2018}
}
@article{Vallet2015,
abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
author = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
doi = {10.1016/j.cag.2015.03.004},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/terraMobilita-iqmulus-urban-point-cloud-analysis-benchmark.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene,database,dataset,georeferenced,scene,synched},
mendeley-tags = {database,dataset,georeferenced,scene,synched},
month = {jun},
pages = {126--133},
publisher = {Pergamon},
title = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S009784931500028X},
volume = {49},
year = {2015}
}
@article{Pomerleau2012,
abstract = {The number of registration solutions in the literature has bloomed recently. The iterative closest point, for example, could be considered as the backbone of many laser-based localization and mapping systems. Although they are widely used, it is a common challenge to compare registration solutions on a fair base. The main limitation is to overcome the lack of accurate ground truth in current data sets, which usually cover environments only over a small range of organization levels. In computer vision, the Stanford 3D Scanning Repository pushed forward point cloud registration algorithms and object modeling fields by providing high-quality scanned objects with precise localization. We aim to provide similar high-caliber working material to the robotic and computer vision communities but with sceneries instead of objects. We propose eight point cloud sequences acquired in locations covering the environment diversity that modern robots are susceptible to encounter, ranging from inside an apartment to a woodland area. The core of the data sets consists of 3D laser point clouds for which supporting data (Gravity, Magnetic North and GPS) are given for each pose. A special effort has been made to ensure global positioning of the scanner within mm-range precision, independent of environmental conditions. This will allow for the development of improved registration algorithms when mapping challenging environments, such as those found in real-world situations.1},
author = {Pomerleau, Fran{\c{c}}ois and Liu, Ming and Colas, Francis and Siegwart, Roland},
doi = {10.1177/0278364912458814},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pomerleau et al. - Unknown - Challenging data sets for point cloud registration algorithms.pdf:pdf},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Field robots,computer vision,database,dataset,dynamic,field and service robotics,human,range sensing,scene,search and rescue robots,sensing and perception},
mendeley-tags = {database,dataset,dynamic,human,scene},
number = {14},
pages = {1705--1711},
title = {{Challenging data sets for point cloud registration algorithms}},
url = {https://journals.sagepub.com/doi/abs/10.1177/0278364912458814},
volume = {31},
year = {2012}
}
@inproceedings{Abramov2012,
abstract = {We present a real-time technique for the spatiotemporal segmentation of color/depth movies. Images are segmented using a parallel Metropolis algorithm implemented on a GPU utilizing both color and depth information, acquired with the Microsoft Kinect. Segments represent the equilibrium states of a Potts model, where tracking of segments is achieved by warping obtained segment labels to the next frame using real-time optical flow, which reduces the number of iterations required for the Metropolis method to encounter the new equilibrium state. By including depth information into the framework, true objects boundaries can be found more easily, improving also the temporal coherency of the method. The algorithm has been tested for videos of medium resolutions showing human manipulations of objects. The framework provides an inexpensive visual front end for visual preprocessing of videos in industrial settings and robot labs which can potentially be used in various applications.},
author = {Abramov, Alexey and Pauwels, Karl and Papon, Jeremie and Worgotter, Florentin and Dellen, Babette},
booktitle = {Proceedings of IEEE Workshop on Applications of Computer Vision},
doi = {10.1109/WACV.2012.6163000},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abramov et al. - Unknown - Depth-supported real-time video segmentation with the Kinect.pdf:pdf},
isbn = {9781467302333},
issn = {21583978},
keywords = {action,dataset,human,manipulation,synched},
mendeley-tags = {action,dataset,human,manipulation,synched},
pages = {457--464},
title = {{Depth-supported real-time video segmentation with the Kinect}},
url = {https://ieeexplore.ieee.org/abstract/document/6163000/},
year = {2012}
}
@inproceedings{Munoz2009,
abstract = {We address the problem of label assignment in computer vision: given a novel 3D or 2D scene, we wish to assign a unique label to every site (voxel, pixel, superpixel, etc.). To this end, the Markov Random Field framework has proven to be a model of choice as it uses contextual information to yield improved classification results over locally independent classifiers. In this work we adapt a functional gradient approach for learning high-dimensional parameters of random fields in order to perform discrete, multi-label classification. With this approach we can learn robust models involving high-order interactions better than the previously used learning method. We validate the approach in the context of point cloud classification and improve the state of the art. In addition, we successfully demonstrate the generality of the approach on the challenging vision problem of recovering 3-D geometric surfaces from images.},
author = {Munoz, Daniel and Bagnell, J. Andrew and Vandapel, Nicolas and Hebert, Martial},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206590},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/contextual-classification-with-functional-max-margin-markov-networks.pdf:pdf},
isbn = {9781424439935},
keywords = {database,dataset,scene,synched},
mendeley-tags = {database,dataset,scene,synched},
pages = {975--982},
title = {{Contextual classification with functional max-margin markov networks}},
url = {https://www.google.com/search?client=ubuntu{\&}channel=fs{\&}q=Contextual+Classification+with+Functional+Max-Margin+Markov+Networks{\&}ie=utf-8{\&}oe=utf-8},
volume = {2009 IEEE},
year = {2009}
}
@article{Deuge2013,
author = {{De Deuge}, Mark and Quadros, A and Hung, C and Douillard, B},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deuge et al. - Unknown - Unsupervised feature learning for classification of outdoor 3d scans(2).pdf:pdf},
journal = {Australasian Conf. Robotics Automat. (ACRA)},
keywords = {database,dataset,dynamic,scene,synched},
mendeley-tags = {database,dataset,dynamic,scene,synched},
pages = {2--4},
title = {{Unsupervised feature learning for classification of outdoor 3d scans BT - Australasian Conference on Robitics and Automation}},
url = {http://www.araa.asn.au/acra/acra2013/papers/pap133s1-file1.pdf},
volume = {2},
year = {2013}
}
@article{Ariz2016,
abstract = {A new public database of videos for head tracking and pose estimation is presented in this paper with the goal of establishing a new framework for algorithm validation, replacing out of date frameworks. Position data has been recorded with a magnetic sensor-transmitter that has previously been aligned and synchronized with a commercial webcam, and we provide reliable ground-truth for 3D rotation and translation of the head with respect to the camera. In addition to this, an automatic face annotation procedure has been developed, which provides the image position of 54 facial landmarks, with negligible error, in every video frame in the database. This image ground-truth can be used for algorithm training or head tracking evaluation, among others. In order to show the usability of the database, we evaluate three head tracking approaches and three head models, and combine them to provide nine different head pose estimation sets of results. We show the validity of the presented database both for training and evaluation of head tracking and pose estimation methods, and provide an interesting comparison in performance of state-of-the-art algorithms. These results may also serve as reference to encourage other researchers to train and test their algorithms with this database, and compare their results with the ones presented in this paper.},
author = {Ariz, Mikel and Bengoechea, Jos{\'{e}} J. and Villanueva, Arantxa and Cabeza, Rafael},
doi = {10.1016/j.cviu.2015.04.009},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/a-novel-2d3d-database-with-automatic-face-annotation-for-head-tracking-and-pose-estimation.pdf:pdf},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {AAM,ASM,Automatic face annotation,Head model,Head pose estimation,Head tracking,POSIT},
pages = {201--210},
title = {{A novel 2D/3D database with automatic face annotation for head tracking and pose estimation}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314215000934},
volume = {148},
year = {2016}
}
@inproceedings{Hackel,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
author = {Hackel, T and Savinov, N and Ladicky, L and Wegner, J. D. and Schindler, K. and Pollefeys, M.},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-91-2017},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hackel et al. - 2017 - SEMANTIC3D.NET A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK.pdf:pdf},
issn = {21949050},
keywords = {database,dataset,scene,synched},
mendeley-tags = {database,dataset,scene,synched},
number = {1W1},
pages = {91--98},
title = {{SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK}},
url = {https://arxiv.org/abs/1704.03847},
volume = {4},
year = {2017}
}
@inproceedings{Firman2016,
abstract = {Since the launch of the Microsoft Kinect, scores of RGBD datasets have been released. These have propelled advances in areas from reconstruction to gesture recognition. In this paper we explore the field, reviewing datasets across eight categories: semantics, object pose estimation, camera tracking, scene reconstruction, object tracking, human actions, faces and identification. By extracting relevant information in each category we help researchers to find appropriate data for their needs, and we consider which datasets have succeeded in driving computer vision forward and why. Finally, we examine the future of RGBD datasets. We identify key areas which are currently underexplored, and suggest that future directions may include synthetic data and dense reconstructions of static and dynamic scenes.},
author = {Firman, Michael},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.88},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/computer, 2016 - Unknown - RGBD datasets Past, present and future.pdf:pdf},
isbn = {9781467388504},
issn = {21607516},
number = {Section 3},
pages = {661--673},
title = {{RGBD Datasets: Past, Present and Future}},
url = {https://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016{\_}workshops/w17/html/Firman{\_}RGBD{\_}Datasets{\_}Past{\_}CVPR{\_}2016{\_}paper.html},
year = {2016}
}
@article{Gasparrini2014,
abstract = {We propose an automatic, privacy-preserving, fall detection method for indoor environments, based on the usage of the Microsoft Kinect{\textregistered} depth sensor, in an "on-ceiling" configuration, and on the analysis of depth frames. All the elements captured in the depth scene are recognized by means of an Ad-Hoc segmentation algorithm, which analyzes the raw depth data directly provided by the sensor. The system extracts the elements, and implements a solution to classify all the blobs in the scene. Anthropometric relationships and features are exploited to recognize one or more human subjects among the blobs. Once a person is detected, he is followed by a tracking algorithm between different frames. The use of a reference depth frame, containing the set-up of the scene, allows one to extract a human subject, even when he/she is interacting with other objects, such as chairs or desks. In addition, the problem of blob fusion is taken into account and efficiently solved through an inter-frame processing algorithm. A fall is detected if the depth blob associated to a person is near to the floor. Experimental tests show the effectiveness of the proposed solution, even in complex scenarios. {\textcopyright} 2014 by the authors; licensee MDPI, Basel, Switzerland.},
author = {Gasparrini, Samuele and Cippitelli, Enea and Spinsante, Susanna and Gambi, Ennio},
doi = {10.3390/s140202756},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gasparrini et al. - Unknown - A depth-based fall detection system using a Kinect{\textregistered} sensor.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Depth frame,Elderly care,Fall detection,Human recognition,Kinect,dataset,falling,human},
mendeley-tags = {dataset,falling,human},
number = {2},
pages = {2756--2775},
title = {{A depth-based fall detection system using a Kinect{\textregistered} sensor}},
url = {https://www.mdpi.com/1424-8220/14/2/2756},
volume = {14},
year = {2014}
}
@inproceedings{Yun2012,
abstract = {Human activity recognition has potential to impact a wide range of applications from surveillance to human computer interfaces to content based video retrieval. Recently, the rapid development of inexpensive depth sensors (e.g. Microsoft Kinect) provides adequate accuracy for real-time full-body human tracking for activity recognition applications. In this paper, we create a complex human activity dataset depicting two person interactions, including synchronized video, depth and motion capture data. Moreover, we use our dataset to evaluate various features typically used for indexing and retrieval of motion capture data, in the context of real-time detection of interaction activities via Support Vector Machines (SVMs). Experimentally, we find that the geometric relational features based on distance between all pairs of joints outperforms other feature choices. For whole sequence classification, we also explore techniques related to Multiple Instance Learning (MIL) in which the sequence is represented by a bag of body-pose features. We find that the MIL based classifier outperforms SVMs when the sequences extend temporally around the interaction of interest.},
author = {Yun, Kiwon and Honorio, Jean and Chattopadhyay, Debaleena and Berg, Tamara L. and Samaras, Dimitris},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2012.6239234},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yun et al. - Unknown - Two-person interaction detection using body-pose features and multiple instance learning.pdf:pdf},
isbn = {9781467316118},
issn = {21607508},
keywords = {dataset,dynamic,human,synched},
mendeley-tags = {dataset,dynamic,human,synched},
pages = {28--35},
title = {{Two-person interaction detection using body-pose features and multiple instance learning}},
url = {https://ieeexplore.ieee.org/abstract/document/6239234/},
year = {2012}
}
@inproceedings{Cherabier2016,
abstract = {Techniques that jointly perform dense 3D reconstruction and semantic segmentation have recently shown very promising results. One major restriction so far is that they can often only handle a very low number of semantic labels. This is mostly due to their high memory consumption caused by the necessity to store indicator variables for every label and transition. We propose a way to reduce the memory consumption of existing methods. Our approach is based on the observation that many semantic labels are only present at very localized positions in the scene, such as cars. Therefore this label does not need to be active at every location. We exploit this observation by dividing the scene into blocks in which generally only a subset of labels is active. By determining early on in the reconstruction process which labels need to be active in which block the memory consumption can be significantly reduced. In order to recover from mistakes we propose to update the set of active labels during the iterative optimization procedure based on the current solution. We also propose a way to initialize the set of active labels using a boosted classifier. In our experimental evaluation we show the reduction of memory usage quantitatively. Eventually, we show results of joint semantic 3D reconstruction and semantic segmentation with significantly more labels than previous approaches were able to handle.},
author = {Cherabier, Ian and Hane, Christian and Oswald, Martin R. and Pollefeys, Marc},
booktitle = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
doi = {10.1109/3DV.2016.68},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - Unknown - Pointnet Deep learning on point sets for 3d classification and segmentation.pdf:pdf},
isbn = {9781509054077},
keywords = {dense 3D reconstruction,semantic segmentation},
pages = {601--610},
title = {{Multi-label semantic 3D reconstruction using voxel blocks}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2017/html/Qi{\_}PointNet{\_}Deep{\_}Learning{\_}CVPR{\_}2017{\_}paper.html},
year = {2016}
}
@inproceedings{Bagautdinov2015,
abstract = {Single depth map으로부터 다수 존재 확률을 계산하고, scene 안에 잠재적 요소를 가려 객체를 인식하는 방법 설명(different type object 구별)},
author = {Bagautdinov, Timur and Fleuret, Fran{\c{c}}ois and Fua, Pascal},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298900},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/Bagautdinov{\_}Probability{\_}Occupancy{\_}Maps{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
keywords = {dataset,human,synched},
mendeley-tags = {dataset,human,synched},
month = {jun},
pages = {2829--2837},
publisher = {IEEE},
title = {{Probability occupancy maps for occluded depth images}},
url = {http://ieeexplore.ieee.org/document/7298900/},
volume = {07-12-June},
year = {2015}
}
@article{Shi2006,
abstract = {A random forest (RF) predictor is an ensemble of$\backslash$nindividual tree predictors. As part of their construction,$\backslash$nRF predictors naturally lead to a dissimilarity measure$\backslash$nbetween the observations. One can also define an RF$\backslash$ndissimilarity measure between unlabeled data: the idea is$\backslash$nto construct an RF predictor that distinguishes the$\backslash$nobserved data from suitably generated synthetic data. The$\backslash$nobserved data are the original unlabeled data and the$\backslash$nsynthetic data are drawn from a reference distribution.$\backslash$nHere we describe the properties of the RF dissimilarity and$\backslash$nmake recommendations on how to use it in practice. An RF$\backslash$ndissimilarity can be attractive because it handles mixed$\backslash$nvariable types well, is invariant to monotonic$\backslash$ntransformations of the input variables, and is robust to$\backslash$noutlying observations. The RF dissimilarity easily deals$\backslash$nwith a large number of variables due to its intrinsic$\backslash$nvariable selection; for example, the Addcl 1 RF$\backslash$ndissimilarity weighs the contribution of each variable$\backslash$naccording to how dependent it is on other variables. We$\backslash$nfind that the RF dissimilarity is useful for detecting$\backslash$ntumor sample clusters on the basis of tumor marker$\backslash$nexpressions. In this application, biologically meaningful$\backslash$nclusters can often be described with simple thresholding$\backslash$nrules.},
author = {Shi, Tao and Horvath, Steve},
doi = {10.1198/106186006X94072},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/unsupervised-learning-with-random-forest-predictors.pdf:pdf},
issn = {10618600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Biomarkers,Cluster analysis,Dissimilarity,Ensemble predictors,Tumor markers},
month = {mar},
number = {1},
pages = {118--138},
title = {{Unsupervised learning with random forest predictors}},
url = {https://www.tandfonline.com/doi/full/10.1198/106186006X94072},
volume = {15},
year = {2006}
}
@inproceedings{Kuehne2011,
abstract = {With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.},
author = {Kuehne, H. and Jhuang, H. and Garrote, E. and Poggio, T. and Serre, T.},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2011.6126543},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/HMDB-a-large-video-database-for-human-motion-recognition.pdf:pdf},
isbn = {9781457711015},
keywords = {2D,database,dataset,synched,video},
mendeley-tags = {2D,database,dataset,synched,video},
month = {nov},
pages = {2556--2563},
publisher = {IEEE},
title = {{HMDB: A large video database for human motion recognition}},
url = {http://ieeexplore.ieee.org/document/6126543/},
year = {2011}
}
@inproceedings{Rougier2006,
abstract = {Faced with the growing population of seniors, Western societies need to think about new technologies to ensure the safety of elderly people at home. Computer vision provides a good solution for healthcare systems because it allows a specific analysis of people behavior. Moreover, a system based on video surveillance is particularly well adapted to detect falls. We present a new method to detect falls using a single camera. Our approach is based on the 3D trajectory of the head, which allows us to distinguish falls from normal activities using 3D velocities. {\textcopyright} 2006 IEEE.},
author = {Rougier, Caroline and Meunier, Jean and St-Arnaud, Alain and Rousseau, Jacqueline},
booktitle = {Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
doi = {10.1109/IEMBS.2006.260829},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rougier et al. - Unknown - Monocular 3D head tracking to detect falls of elderly people.pdf:pdf},
isbn = {1424400325},
issn = {05891019},
pages = {6384--6387},
title = {{Monocular 3D head tracking to detect falls of elderly people}},
url = {https://ieeexplore.ieee.org/abstract/document/4463271/},
year = {2006}
}
@techreport{Qi2019,
abstract = {Current 3D object detection methods are heavily influenced by 2D detectors. In order to leverage architectures in 2D detectors, they often convert 3D point clouds to regular grids (i.e., to voxel grids or to bird's eye view images), or rely on detection in 2D images to propose 3D boxes. Few works have attempted to directly detect objects in point clouds. In this work, we return to first principles to construct a 3D detection pipeline for point cloud data and as generic as possible. However, due to the sparse nature of the data -- samples from 2D manifolds in 3D space -- we face a major challenge when directly predicting bounding box parameters from scene points: a 3D object centroid can be far from any surface point thus hard to regress accurately in one step. To address the challenge, we propose VoteNet, an end-to-end 3D object detection network based on a synergy of deep point set networks and Hough voting. Our model achieves state-of-the-art 3D detection on two large datasets of real 3D scans, ScanNet and SUN RGB-D with a simple design, compact model size and high efficiency. Remarkably, VoteNet outperforms previous methods by using purely geometric information without relying on color images.},
archivePrefix = {arXiv},
arxivId = {1904.09664},
author = {Qi, Charles R. and Litany, Or and He, Kaiming and Guibas, Leonidas J.},
eprint = {1904.09664},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qi et al. - 2019 - Deep Hough Voting for 3D Object Detection in Point Clouds.pdf:pdf},
keywords = {important read,methodology},
mendeley-tags = {important read,methodology},
month = {apr},
title = {{Deep Hough Voting for 3D Object Detection in Point Clouds}},
url = {http://arxiv.org/abs/1904.09664},
year = {2019}
}
@inproceedings{Held2013,
abstract = {Precision tracking is important for predicting the behavior of other$\backslash$ncars in autonomous driving. We present a novel method to combine$\backslash$nlaser and camera data to achieve accurate velocity estimates of moving$\backslash$nvehicles. We combine sparse laser points with a high-resolution camera$\backslash$nimage to obtain a dense colored point cloud. We use a color-augmented$\backslash$nsearch algorithm to align the dense color point clouds from successive$\backslash$ntime frames for a moving vehicle, thereby obtaining a precise estimate$\backslash$nof the tracked vehicle's velocity. Using this alignment method, we$\backslash$nobtain velocity estimates at a much higher accuracy than previous$\backslash$nmethods. Through pre-filtering, we are able to achieve near real$\backslash$ntime results. We also present an online method for real-time use$\backslash$nwith accuracies close to that of the full method. We present a novel$\backslash$napproach to quantitatively evaluate our velocity estimates by tracking$\backslash$na parked car in a local reference frame in which it appears to be$\backslash$nmoving relative to the ego vehicle. We use this evaluation method$\backslash$nto automatically quantitatively evaluate our tracking performance$\backslash$non 466 separate tracked vehicles. Our method obtains a mean absolute$\backslash$nvelocity error of 0.27 m/s and an RMS error of 0.47 m/s on this test$\backslash$nset. We can also qualitatively evaluate our method by building color$\backslash$n3D car models from moving vehicles. We have thus demonstrated that$\backslash$nour method can be used for precision car tracking with applications$\backslash$nto autonomous driving and behavior modeling.},
author = {Held, David and Levinson, Jesse and Thrun, Sebastian},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630715},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - Unknown - Precision tracking with sparse 3d and dense color 2d data.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
pages = {1138--1145},
title = {{Precision tracking with sparse 3D and dense color 2D data}},
url = {https://ieeexplore.ieee.org/abstract/document/6630715/},
year = {2013}
}
@article{Knoop2009,
abstract = {In this article, we present an approach for the fusion of 2d and 3d measurements for model-based person tracking, also known as Human Motion Capture. The applied body model is defined geometrically with generalized cylinders, and is set up hierarchically with connecting joints of different types. The joint model can be parameterized to control the degrees of freedom, adhesion and stiffness. This results in an articulated body model with constrained kinematic degrees of freedom. The fusion approach incorporates this model knowledge together with the measurements, and tracks the target body iteratively with an extended Iterative Closest Point (ICP) approach. Generally, the ICP is based on the concept of correspondences between measurements and model, which is normally exploited to incorporate 3d point cloud measurements. The concept has been generalized to represent and incorporate also 2d image space features. Together with the 3D point cloud from a 3d time-of-flight (ToF) camera, arbitrary features, derived from 2D camera images, are used in the fusion algorithm for tracking of the body. This gives complementary information about the tracked body, enabling not only tracking of depth motions but also turning movements of the human body, which is normally a hard problem for markerless human motion capture systems. The resulting tracking system, named VooDoo is used to track humans in a Human-Robot Interaction (HRI) context. We only rely on sensors on board the robot, i.e. the color camera, the ToF camera and a laser range finder. The system runs in realtime (∼20 Hz) and is able to robustly track a human in the vicinity of the robot. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Knoop, Steffen and Vacek, Stefan and Dillmann, R{\"{u}}diger},
doi = {10.1016/j.robot.2008.10.017},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/fusion-of-2d-and-3d-sensor-data-for-articulated-body-tracking.pdf:pdf},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {3D body model,Human motion capture,Human robot interaction,Sensor fusion,Time-of-flight},
number = {3},
pages = {321--329},
title = {{Fusion of 2d and 3d sensor data for articulated body tracking}},
url = {https://www.sciencedirect.com/science/article/pii/S0921889008001711},
volume = {57},
year = {2009}
}
@article{Gorelick,
abstract = {Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure and orientation. We show that these features are useful for action recognition, detection and clustering. The method is fast, does not require video alignment and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action, and low quality video.},
author = {Gorelick, Lena and Blank, Moshe and Shechtman, Eli and Irani, Michal and Basri, Ronen},
doi = {10.1109/TPAMI.2007.70711},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gorelick et al. - Unknown - Actions as space-time shapes.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {2D,Action recognition,Action representation,Poisson equation,Shape analysis,Space-time analysis,action,dataset,human,synched,video},
mendeley-tags = {2D,action,dataset,human,synched,video},
number = {12},
pages = {2247--2253},
title = {{Actions as space-time shapes}},
url = {https://ieeexplore.ieee.org/abstract/document/4359333/},
volume = {29},
year = {2007}
}
@inproceedings{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_54},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Silberman et al. - 2012 - Indoor segmentation and support inference from RGBD images.pdf:pdf},
isbn = {9783642337147},
issn = {03029743},
keywords = {database,dataset,scene,synched},
mendeley-tags = {database,dataset,scene,synched},
number = {PART 5},
pages = {746--760},
title = {{Indoor segmentation and support inference from RGBD images}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-33715-4{\_}54},
volume = {7576 LNCS},
year = {2012}
}
@inproceedings{Koppula2011,
abstract = {RGB-D cameras that give an RGB image together with depth data have become widely available. In this paper, we use this data to build 3D point clouds of full indoor scenes such as an office and address the task of semantic la- beling of these 3D point clouds. We propose a graphical model that captures var- ious features and contextual relations, including the local visual appearance and shape cues, object co-occurence relationships and geometric relationships. With a large number of object classes and relations, the models parsimony becomes im- portant and we address that by using multiple types of edge potentials. The model admits efficient approximate inference, and we train it using a maximum-margin learning approach. In our experiments over a total of 52 3D scenes of homes and offices (composed from about 550 views, having 2495 segments labeled with 27 object classes), we get a performance of 84.06{\%} in labeling 17 object classes for offices, and 73.38{\%} in labeling 17 object classes for home scenes. Finally, we applied these algorithms successfully on a mobile robot for the task of finding objects in large cluttered rooms},
author = {Koppula, Hema Swetha and Anand, Abhishek and Joachims, Thorsten and Saxena, Ashutosh},
booktitle = {Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011, NIPS 2011},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koppula et al. - Unknown - Semantic labeling of 3d point clouds for indoor scenes.pdf:pdf},
isbn = {9781618395993},
title = {{Semantic labeling of 3D point clouds for indoor scenes}},
url = {http://papers.nips.cc/paper/4226-semantic-labeling-of-3d-point-clouds-for-indoor-scenes},
year = {2011}
}
@inproceedings{Hegger2013,
abstract = {The ability to detect people in domestic and unconstrained environments is crucial for every service robot. The knowledge where people are is required to perform several tasks such as navigation with dynamic obstacle avoidance and human-robot-interaction. In this paper we propose a people detection approach based on 3d data provided by a RGB-D camera. We introduce a novel 3d feature descriptor based on Local Surface Normals (LSN) which is used to learn a classifier in a supervised machine learning manner. In order to increase the systems flexibility and to detect people even under partial occlusion we introduce a top-down/bottom-up segmentation. We deployed the people detection system on a real-world service robot operating at a reasonable frame rate of 5Hz. The experimental results show that our approach is able to detect persons in various poses and motions such as sitting, walking, and running.},
author = {Hegger, Frederik and Hochgeschwender, Nico and Kraetzschmar, Gerhard K. and Ploeger, Paul G.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-39250-4_15},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/people-detection-in-3d-point-clouds-using-local-surface-normals.pdf:pdf},
isbn = {9783642392498},
issn = {03029743},
keywords = {Human-Robot Interaction,People Detection,RGB-D},
pages = {154--164},
title = {{People detection in 3d point clouds using local surface normals}},
url = {http://link.springer.com/10.1007/978-3-642-39250-4{\_}15},
volume = {7500 LNAI},
year = {2013}
}
@article{Chaquet2013,
author = {{Jos{\'{e}} M. Chaquet} and {Enrique J. Carmona} and {Antonio Fern{\'{a}}ndez-Caballero}},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaquet, {\ldots}, 2013 - Unknown - A survey of video datasets for human action and activity recognition.pdf:pdf},
journal = {Computer Vision and Image Understanding},
number = {6},
pages = {633--659},
title = {{A survey of video datasets for human action and activity recognition}},
url = {https://www.sciencedirect.com/science/article/pii/S1077314213000295},
volume = {117},
year = {2013}
}
@inproceedings{Liciotti2017,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Video analytics, involves a variety of techniques to monitor, analyse, and extract meaningful information from video streams. In this light, person re-identification is an important topic in scene monitoring, human computer interaction, retail, people counting, ambient assisted living and many other computer vision research. The existing datasets are not suitable for activity monitoring and human behaviour analysis. For this reason we build a novel dataset for person re-identification that uses an RGB-D camera in a top-view configuration. This setup choice is primarily due to the reduction of occlusions and it has also the advantage of being privacy preserving, because faces are not recorded by the camera. The use of an RGB-D camera allows to extract anthropometric features for the recognition of people passing under the camera. The paper describes in details the collection and construction modalities of the dataset TVPR. This is composed by 100 people and for each video frame nine depth and colour features are computed and provided together with key descriptive statistics.},
author = {Liciotti, Daniele and Paolanti, Marina and Frontoni, Emanuele and Mancini, Adriano and Zingaretti, Primo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56687-0_1},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liciotti et al. - Unknown - Person re-identification dataset with rgb-d camera in a top-view configuration.pdf:pdf},
isbn = {9783319566863},
issn = {16113349},
keywords = {Person re-identification,RGB-D camera,TVPR,Top-view dataset,dataset,dynamic,human},
mendeley-tags = {dataset,dynamic,human},
pages = {1--11},
title = {{Person re-identification dataset with RGB-D camera in a top-view configuration}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-56687-0{\_}1},
volume = {10165 LNCS},
year = {2017}
}
@inproceedings{Diby2015,
abstract = {— Human pose estimation in realistic world condi-tions raises multiple challenges such as foreground extraction, background update and occlusion by scene objects. Most of existing approaches were demonstrated in controlled environ-ments. In this paper, we propose a framework to improve the performance of existing tracking methods to cope with these problems. To this end, a robust and scalable framework is provided composed of three main stages. In the first one, a probabilistic occupancy grid updated with a Hidden Markov Model used to maintain an up-to-date background and to extract moving persons. The second stage uses component labelling to identify and track persons in the scene. The last stage uses an hierarchical particle filter to estimate the body pose for each moving person. Occlusions are handled by querying the occupancy grid to identify hidden body parts so that they can be discarded from the pose estimation process. We provide a parallel implementation that runs on CPU and GPU at 4 frames per second. We also validate the approach on our own dataset that consists of synchronized motion capture with a single RGB-D camera data of a person performing ac-tions in challenging situations with severe occlusions generated by scene objects. We make this dataset available online.},
author = {Diby, Abdallah and Charpillety, Fran{\c{c}}ois},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7354068},
file = {:home/rais/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dib, (IROS), 2015 - Unknown - Pose estimation for a partially observable human body from RGB-D cameras.pdf:pdf},
isbn = {9781479999941},
issn = {21530866},
keywords = {Cameras,Data models,Hidden Markov models,Sensors,Solid modeling,Three-dimensional displays,action,dataset,human,synched},
mendeley-tags = {action,dataset,human,synched},
pages = {4915--4922},
title = {{Pose estimation for a partially observable human body from RGB-D cameras}},
url = {https://ieeexplore.ieee.org/abstract/document/7354068/},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Chen2015,
abstract = {Human action recognition has a wide range of applications in-cluding biometrics, surveillance, and human computer interaction. The use of multimodal sensors for human action recognition is steadily increasing. However, there are limited publicly available datasets where depth camera and inertial sensor data are captured at the same time. This paper describes a freely available dataset, named UTD-MHAD, which consists of four temporally synchro-nized data modalities. These modalities include RGB videos, depth videos, skeleton positions, and inertial signals from a Kinect cam-era and a wearable inertial sensor for a comprehensive set of 27 human actions. Experimental results are provided to show how this database can be used to study fusion approaches that involve using both depth camera data and inertial sensor data. This public do-main dataset is of benefit to multimodality research activities being conducted for human action recognition by various research groups.},
author = {Chen, Chen and Jafari, Roozbeh and Kehtarnavaz, Nasser},
booktitle = {Proceedings - International Conference on Image Processing, ICIP},
doi = {10.1109/ICIP.2015.7350781},
file = {:home/rais/Documents/thesis/paper-writing/2d-tracking-for-3d-maps/UTD-MHAD-a-multimodal-dataset-for-human-action-recognition-utilizing-a-depth-camera-and-a-wearable-inertial-sensor.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
keywords = {Multimodal human action dataset,action,dataset,fusion of depth and inertial data,human,human action recognition,synched},
mendeley-tags = {action,dataset,human,synched},
month = {sep},
pages = {168--172},
publisher = {IEEE},
title = {{UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor}},
url = {http://ieeexplore.ieee.org/document/7350781/},
volume = {2015-Decem},
year = {2015}
}
